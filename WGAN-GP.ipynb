{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:03:40.514630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5111:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5111:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5111:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5599:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2660:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5111:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5111:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5111:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5599:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2660:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import imageio\n",
    "import moviepy.editor as mpy\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # disable warnings and info\n",
    "SAMPLE_COL = 16\n",
    "SAMPLE_ROW = 16\n",
    "SAMPLE_NUM = SAMPLE_COL * SAMPLE_ROW\n",
    "\n",
    "IMG_H = 64\n",
    "IMG_W = 64\n",
    "IMG_C = 3\n",
    "IMG_SHAPE = (IMG_H, IMG_W, IMG_C)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "BZ = (BATCH_SIZE, Z_DIM)\n",
    "BUF = 65536\n",
    "\n",
    "\n",
    "W_LR_D = 3e-4\n",
    "W_LR_G = 1.5e-4\n",
    "W_EPOCH = 300\n",
    "\n",
    "BUFFER_SIZE = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:03:42.100378: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:03:42.101575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-25 16:03:42.129708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-01-25 16:03:42.130766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-01-25 16:03:42.131487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:85:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-01-25 16:03:42.132525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:86:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-01-25 16:03:42.132562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-25 16:03:42.138097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-01-25 16:03:42.138153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-01-25 16:03:42.140360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-25 16:03:42.140747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-25 16:03:42.143102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-25 16:03:42.144276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-01-25 16:03:42.144467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-01-25 16:03:42.152059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-01-25 16:03:42.156668: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-25 16:03:42.157940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:86:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-01-25 16:03:42.157979: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-25 16:03:42.158008: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-01-25 16:03:42.158035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-01-25 16:03:42.158061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-25 16:03:42.158086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-25 16:03:42.158112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-25 16:03:42.158137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-01-25 16:03:42.158163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-01-25 16:03:42.160441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 3\n",
      "2022-01-25 16:03:42.160482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-25 16:03:42.898726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-25 16:03:42.898765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      3 \n",
      "2022-01-25 16:03:42.898773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   N \n",
      "2022-01-25 16:03:42.902053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pci bus id: 0000:86:00.0, compute capability: 6.0)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n",
      "000001.png\n",
      "./img_align_celeba_png/000001.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_list = os.listdir('./img_align_celeba_png/')\n",
    "print(len(file_list))\n",
    "print(file_list[0])\n",
    "print('./img_align_celeba_png/%s' % file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name):\n",
    "    img = tf.io.read_file('./img_align_celeba_png/' + img_name)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    img = tf.image.resize(img, (IMG_H, IMG_W))\n",
    "    img = (img / 255) # change range \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((file_list))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def utPuzzle(imgs, row, col, path=None):\n",
    "    h, w, c = imgs[0].shape\n",
    "    out = np.zeros((h * row, w * col, c), np.uint8)\n",
    "    for n, img in enumerate(imgs):\n",
    "        j, i = divmod(n, col)\n",
    "        out[j * h : (j + 1) * h, i * w : (i + 1) * w, :] = img\n",
    "#     if path is not None : \n",
    "    imageio.imwrite(path, out)\n",
    "    return out\n",
    "  \n",
    "def utMakeGif(imgs, fname, duration):\n",
    "    n = float(len(imgs)) / duration\n",
    "    clip = mpy.VideoClip(lambda t : imgs[int(n * t)], duration = duration)\n",
    "    clip.write_gif(fname, fps = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN(img_shape, z_dim):\n",
    "    # x-shape\n",
    "    xh, xw, xc = img_shape\n",
    "    # z-shape\n",
    "    zh = xh // 4\n",
    "    zw = xw // 4\n",
    "        \n",
    "    # return Generator and Discriminator\n",
    "    return keras.Sequential([ # Generator\n",
    "        keras.layers.Dense(units  =  1024, input_shape = (z_dim,)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Dense(units  =  zh * zw << 8), # zh * zw * 256\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Reshape(target_shape = (zh, zw, 256)),\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters = 32,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters = xc,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\",\n",
    "            activation = keras.activations.sigmoid\n",
    "        ),\n",
    "    ]), keras.Sequential([ # Discriminator\n",
    "        keras.layers.Conv2D(\n",
    "            filters = 32,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\",\n",
    "            input_shape = img_shape,\n",
    "        ),\n",
    "        keras.layers.LeakyReLU(),\n",
    "        keras.layers.Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\"\n",
    "        ),\n",
    "        keras.layers.LeakyReLU(),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(units  =  1024),\n",
    "        keras.layers.LeakyReLU(),\n",
    "        keras.layers.Dense(units  =  1),\n",
    "    ])\n",
    "\n",
    "s = tf.random.normal([SAMPLE_NUM, Z_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WG, WD = GAN(IMG_SHAPE, Z_DIM)\n",
    "optimizer_g = keras.optimizers.Adam(W_LR_G, beta_1=0., beta_2=0.9)\n",
    "optimizer_d = keras.optimizers.Adam(W_LR_D, beta_1=0., beta_2=0.9)\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(WG_net=WG, WD_net=WD, optimizer_g=optimizer_g, optimizer_d=optimizer_d)\n",
    "\n",
    "manager = tf.train.CheckpointManager(ckpt, './ckpts', max_to_keep=10,\n",
    "                                     checkpoint_name='WGAN')\n",
    "# ckpt.restore('./ckpts/WGAN-1')\n",
    "\n",
    "@tf.function\n",
    "def WGTrain(c1):\n",
    "    z = tf.random.normal(BZ)\n",
    "    with tf.GradientTape() as tpd:\n",
    "        \n",
    "        fake_img = WG(z, training = True)\n",
    "        real_img = c1\n",
    "        epsilon = tf.random.uniform((BATCH_SIZE, 1, 1, 1), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "        c2 = epsilon * real_img + (1-epsilon) * fake_img\n",
    "\n",
    "        z1 = WD(real_img, training = True)\n",
    "        z0 = WD(fake_img, training = True)\n",
    "        with tf.GradientTape() as tpd2:\n",
    "            tpd2.watch(c2)\n",
    "            z2 = WD(c2, training = True)\n",
    "            \n",
    "\n",
    "        gradient_c2 = tpd2.gradient(z2, c2)\n",
    "        gradient_norm =tf.math.sqrt(tf.reduce_sum(tf.square(gradient_c2), axis=(1, 2, 3)))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(gradient_norm - 1.0))\n",
    "        \n",
    "        lg = - tf.reduce_mean(z0)\n",
    "        ld = tf.reduce_mean(z0) - tf.reduce_mean(z1) \\\n",
    "         + 10 * gradient_penalty\n",
    "\n",
    "    gradient_g = tpd.gradient(lg, WG.trainable_variables)\n",
    "\n",
    "    optimizer_g.apply_gradients(zip(gradient_g, WG.trainable_variables))\n",
    "    \n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def WDTrain(c1):\n",
    "    z = tf.random.normal(BZ)\n",
    "    with tf.GradientTape() as tpd:\n",
    "        \n",
    "        fake_img = WG(z, training = True)\n",
    "        real_img = c1\n",
    "        epsilon = tf.random.uniform((BATCH_SIZE, 1, 1, 1), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "        c2 = epsilon * real_img + (1-epsilon) * fake_img\n",
    "\n",
    "        z1 = WD(real_img, training = True)\n",
    "        z0 = WD(fake_img, training = True)\n",
    "        with tf.GradientTape() as tpd2:\n",
    "            tpd2.watch(c2)\n",
    "            z2 = WD(c2, training = True)\n",
    "            \n",
    "\n",
    "        \n",
    "        gradient_c2 = tpd2.gradient(z2, c2)\n",
    "        gradient_norm = tf.math.sqrt(tf.reduce_sum(tf.square(gradient_c2), axis=(1, 2, 3)))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(gradient_norm - 1.0))\n",
    "        \n",
    "        lg = - tf.reduce_mean(z0)\n",
    "        ld = tf.reduce_mean(z0) - tf.reduce_mean(z1) \\\n",
    "         + 10 * gradient_penalty\n",
    "                \n",
    "    gradient_d = tpd.gradient(ld, WD.trainable_variables)\n",
    "\n",
    "    optimizer_d.apply_gradients(zip(gradient_d, WD.trainable_variables))\n",
    "    \n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WTrain = (\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WGTrain\n",
    ")\n",
    "\n",
    "WCritic = len(WTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:03:43.735401: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-25 16:03:43.753365: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000890000 Hz\n",
      "2022-01-25 16:03:49.034347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-01-25 16:03:49.887465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-01-25 16:03:50.175004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "generator loss -41.078419 \n",
      "disciminator loss -11.376164\n",
      "\n",
      "\n",
      "Epoch 1:\n",
      "generator loss -32.170516 \n",
      "disciminator loss -3.078866\n",
      "\n",
      "\n",
      "Epoch 2:\n",
      "generator loss -15.316301 \n",
      "disciminator loss -2.904508\n",
      "\n",
      "\n",
      "Epoch 3:\n",
      "generator loss -7.861407 \n",
      "disciminator loss -3.073332\n",
      "\n",
      "\n",
      "Epoch 4:\n",
      "generator loss -3.165942 \n",
      "disciminator loss -3.169383\n",
      "\n",
      "\n",
      "Epoch 5:\n",
      "generator loss -1.348848 \n",
      "disciminator loss -3.186712\n",
      "\n",
      "\n",
      "Epoch 6:\n",
      "generator loss -0.913736 \n",
      "disciminator loss -3.117279\n",
      "\n",
      "\n",
      "Epoch 7:\n",
      "generator loss -0.358197 \n",
      "disciminator loss -3.023639\n",
      "\n",
      "\n",
      "Epoch 8:\n",
      "generator loss -0.388839 \n",
      "disciminator loss -2.941302\n",
      "\n",
      "\n",
      "Epoch 9:\n",
      "generator loss -0.694144 \n",
      "disciminator loss -2.866858\n",
      "\n",
      "\n",
      "Epoch 10:\n",
      "generator loss -0.797079 \n",
      "disciminator loss -2.797950\n",
      "\n",
      "\n",
      "Epoch 11:\n",
      "generator loss -0.714354 \n",
      "disciminator loss -2.729118\n",
      "\n",
      "\n",
      "Epoch 12:\n",
      "generator loss -0.715142 \n",
      "disciminator loss -2.673888\n",
      "\n",
      "\n",
      "Epoch 13:\n",
      "generator loss -0.577154 \n",
      "disciminator loss -2.622777\n",
      "\n",
      "\n",
      "Epoch 14:\n",
      "generator loss -0.646519 \n",
      "disciminator loss -2.575926\n",
      "\n",
      "\n",
      "Epoch 15:\n",
      "generator loss -0.694380 \n",
      "disciminator loss -2.535392\n",
      "\n",
      "\n",
      "Epoch 16:\n",
      "generator loss -0.757414 \n",
      "disciminator loss -2.501809\n",
      "\n",
      "\n",
      "Epoch 17:\n",
      "generator loss -0.649252 \n",
      "disciminator loss -2.454741\n",
      "\n",
      "\n",
      "Epoch 18:\n",
      "generator loss -0.470856 \n",
      "disciminator loss -2.426055\n",
      "\n",
      "\n",
      "Epoch 19:\n",
      "generator loss -0.345428 \n",
      "disciminator loss -2.388677\n",
      "\n",
      "\n",
      "Epoch 20:\n",
      "generator loss -0.514976 \n",
      "disciminator loss -2.363327\n",
      "\n",
      "\n",
      "Epoch 21:\n",
      "generator loss -0.587169 \n",
      "disciminator loss -2.334262\n",
      "\n",
      "\n",
      "Epoch 22:\n",
      "generator loss -0.568558 \n",
      "disciminator loss -2.306355\n",
      "\n",
      "\n",
      "Epoch 23:\n",
      "generator loss -0.468166 \n",
      "disciminator loss -2.271607\n",
      "\n",
      "\n",
      "Epoch 24:\n",
      "generator loss -0.479613 \n",
      "disciminator loss -2.252998\n",
      "\n",
      "\n",
      "Epoch 25:\n",
      "generator loss -0.490286 \n",
      "disciminator loss -2.223689\n",
      "\n",
      "\n",
      "Epoch 26:\n",
      "generator loss -0.473174 \n",
      "disciminator loss -2.204065\n",
      "\n",
      "\n",
      "Epoch 27:\n",
      "generator loss -0.436269 \n",
      "disciminator loss -2.183798\n",
      "\n",
      "\n",
      "Epoch 28:\n",
      "generator loss -0.453308 \n",
      "disciminator loss -2.161911\n",
      "\n",
      "\n",
      "Epoch 29:\n",
      "generator loss -0.505801 \n",
      "disciminator loss -2.134664\n",
      "\n",
      "\n",
      "Epoch 30:\n",
      "generator loss -0.380691 \n",
      "disciminator loss -2.119663\n",
      "\n",
      "\n",
      "Epoch 31:\n",
      "generator loss -0.506536 \n",
      "disciminator loss -2.103820\n",
      "\n",
      "\n",
      "Epoch 32:\n",
      "generator loss -0.536459 \n",
      "disciminator loss -2.077234\n",
      "\n",
      "\n",
      "Epoch 33:\n",
      "generator loss -0.623618 \n",
      "disciminator loss -2.060948\n",
      "\n",
      "\n",
      "Epoch 34:\n",
      "generator loss -0.723459 \n",
      "disciminator loss -2.045195\n",
      "\n",
      "\n",
      "Epoch 35:\n",
      "generator loss -0.753698 \n",
      "disciminator loss -2.018154\n",
      "\n",
      "\n",
      "Epoch 36:\n",
      "generator loss -0.880476 \n",
      "disciminator loss -1.995028\n",
      "\n",
      "\n",
      "Epoch 37:\n",
      "generator loss -0.960429 \n",
      "disciminator loss -1.974025\n",
      "\n",
      "\n",
      "Epoch 38:\n",
      "generator loss -1.097102 \n",
      "disciminator loss -1.969932\n",
      "\n",
      "\n",
      "Epoch 39:\n",
      "generator loss -1.096119 \n",
      "disciminator loss -1.943757\n",
      "\n",
      "\n",
      "Epoch 40:\n",
      "generator loss -1.236072 \n",
      "disciminator loss -1.926760\n",
      "\n",
      "\n",
      "Epoch 41:\n",
      "generator loss -1.201099 \n",
      "disciminator loss -1.909449\n",
      "\n",
      "\n",
      "Epoch 42:\n",
      "generator loss -1.278605 \n",
      "disciminator loss -1.903861\n",
      "\n",
      "\n",
      "Epoch 43:\n",
      "generator loss -1.254963 \n",
      "disciminator loss -1.881239\n",
      "\n",
      "\n",
      "Epoch 44:\n",
      "generator loss -1.265297 \n",
      "disciminator loss -1.865004\n",
      "\n",
      "\n",
      "Epoch 45:\n",
      "generator loss -1.305803 \n",
      "disciminator loss -1.854130\n",
      "\n",
      "\n",
      "Epoch 46:\n",
      "generator loss -1.408804 \n",
      "disciminator loss -1.842921\n",
      "\n",
      "\n",
      "Epoch 47:\n",
      "generator loss -1.475426 \n",
      "disciminator loss -1.826374\n",
      "\n",
      "\n",
      "Epoch 48:\n",
      "generator loss -1.469017 \n",
      "disciminator loss -1.812358\n",
      "\n",
      "\n",
      "Epoch 49:\n",
      "generator loss -1.535381 \n",
      "disciminator loss -1.796909\n",
      "\n",
      "\n",
      "Epoch 50:\n",
      "generator loss -1.678726 \n",
      "disciminator loss -1.785334\n",
      "\n",
      "\n",
      "Epoch 51:\n",
      "generator loss -1.721618 \n",
      "disciminator loss -1.766209\n",
      "\n",
      "\n",
      "Epoch 52:\n",
      "generator loss -1.696641 \n",
      "disciminator loss -1.754844\n",
      "\n",
      "\n",
      "Epoch 53:\n",
      "generator loss -1.849604 \n",
      "disciminator loss -1.746005\n",
      "\n",
      "\n",
      "Epoch 54:\n",
      "generator loss -1.880508 \n",
      "disciminator loss -1.731873\n",
      "\n",
      "\n",
      "Epoch 55:\n",
      "generator loss -1.934281 \n",
      "disciminator loss -1.726391\n",
      "\n",
      "\n",
      "Epoch 56:\n",
      "generator loss -1.924396 \n",
      "disciminator loss -1.717999\n",
      "\n",
      "\n",
      "Epoch 57:\n",
      "generator loss -1.989767 \n",
      "disciminator loss -1.701482\n",
      "\n",
      "\n",
      "Epoch 58:\n",
      "generator loss -2.013938 \n",
      "disciminator loss -1.693807\n",
      "\n",
      "\n",
      "Epoch 59:\n",
      "generator loss -2.042780 \n",
      "disciminator loss -1.674430\n",
      "\n",
      "\n",
      "Epoch 60:\n",
      "generator loss -1.966784 \n",
      "disciminator loss -1.668673\n",
      "\n",
      "\n",
      "Epoch 61:\n",
      "generator loss -2.123284 \n",
      "disciminator loss -1.659101\n",
      "\n",
      "\n",
      "Epoch 62:\n",
      "generator loss -2.106426 \n",
      "disciminator loss -1.650299\n",
      "\n",
      "\n",
      "Epoch 63:\n",
      "generator loss -2.162463 \n",
      "disciminator loss -1.640005\n",
      "\n",
      "\n",
      "Epoch 64:\n",
      "generator loss -2.122543 \n",
      "disciminator loss -1.627610\n",
      "\n",
      "\n",
      "Epoch 65:\n",
      "generator loss -2.102684 \n",
      "disciminator loss -1.616871\n",
      "\n",
      "\n",
      "Epoch 66:\n",
      "generator loss -2.179891 \n",
      "disciminator loss -1.606084\n",
      "\n",
      "\n",
      "Epoch 67:\n",
      "generator loss -2.306384 \n",
      "disciminator loss -1.600127\n",
      "\n",
      "\n",
      "Epoch 68:\n",
      "generator loss -2.318423 \n",
      "disciminator loss -1.589483\n",
      "\n",
      "\n",
      "Epoch 69:\n",
      "generator loss -2.398167 \n",
      "disciminator loss -1.582866\n",
      "\n",
      "\n",
      "Epoch 70:\n",
      "generator loss -2.526173 \n",
      "disciminator loss -1.576219\n",
      "\n",
      "\n",
      "Epoch 71:\n",
      "generator loss -2.792001 \n",
      "disciminator loss -1.565214\n",
      "\n",
      "\n",
      "Epoch 72:\n",
      "generator loss -2.672925 \n",
      "disciminator loss -1.550567\n",
      "\n",
      "\n",
      "Epoch 73:\n",
      "generator loss -2.796611 \n",
      "disciminator loss -1.547332\n",
      "\n",
      "\n",
      "Epoch 74:\n",
      "generator loss -2.700216 \n",
      "disciminator loss -1.539768\n",
      "\n",
      "\n",
      "Epoch 75:\n",
      "generator loss -2.636516 \n",
      "disciminator loss -1.527666\n",
      "\n",
      "\n",
      "Epoch 76:\n",
      "generator loss -2.760610 \n",
      "disciminator loss -1.517330\n",
      "\n",
      "\n",
      "Epoch 77:\n",
      "generator loss -2.819855 \n",
      "disciminator loss -1.513313\n",
      "\n",
      "\n",
      "Epoch 78:\n",
      "generator loss -2.747001 \n",
      "disciminator loss -1.505567\n",
      "\n",
      "\n",
      "Epoch 79:\n",
      "generator loss -2.662367 \n",
      "disciminator loss -1.499022\n",
      "\n",
      "\n",
      "Epoch 80:\n",
      "generator loss -2.733754 \n",
      "disciminator loss -1.490060\n",
      "\n",
      "\n",
      "Epoch 81:\n",
      "generator loss -2.716148 \n",
      "disciminator loss -1.486450\n",
      "\n",
      "\n",
      "Epoch 82:\n",
      "generator loss -2.734007 \n",
      "disciminator loss -1.475347\n",
      "\n",
      "\n",
      "Epoch 83:\n",
      "generator loss -2.734521 \n",
      "disciminator loss -1.471257\n",
      "\n",
      "\n",
      "Epoch 84:\n",
      "generator loss -2.790750 \n",
      "disciminator loss -1.468647\n",
      "\n",
      "\n",
      "Epoch 85:\n",
      "generator loss -2.832546 \n",
      "disciminator loss -1.455505\n",
      "\n",
      "\n",
      "Epoch 86:\n",
      "generator loss -2.841686 \n",
      "disciminator loss -1.454464\n",
      "\n",
      "\n",
      "Epoch 87:\n",
      "generator loss -2.881114 \n",
      "disciminator loss -1.448928\n",
      "\n",
      "\n",
      "Epoch 88:\n",
      "generator loss -2.953068 \n",
      "disciminator loss -1.441833\n",
      "\n",
      "\n",
      "Epoch 89:\n",
      "generator loss -2.814724 \n",
      "disciminator loss -1.427610\n",
      "\n",
      "\n",
      "Epoch 90:\n",
      "generator loss -2.938509 \n",
      "disciminator loss -1.424689\n",
      "\n",
      "\n",
      "Epoch 91:\n",
      "generator loss -2.796901 \n",
      "disciminator loss -1.417483\n",
      "\n",
      "\n",
      "Epoch 92:\n",
      "generator loss -2.849254 \n",
      "disciminator loss -1.411903\n",
      "\n",
      "\n",
      "Epoch 93:\n",
      "generator loss -2.901216 \n",
      "disciminator loss -1.413173\n",
      "\n",
      "\n",
      "Epoch 94:\n",
      "generator loss -2.920552 \n",
      "disciminator loss -1.404921\n",
      "\n",
      "\n",
      "Epoch 95:\n",
      "generator loss -2.973454 \n",
      "disciminator loss -1.391271\n",
      "\n",
      "\n",
      "Epoch 96:\n",
      "generator loss -2.953133 \n",
      "disciminator loss -1.388725\n",
      "\n",
      "\n",
      "Epoch 97:\n",
      "generator loss -2.890116 \n",
      "disciminator loss -1.391345\n",
      "\n",
      "\n",
      "Epoch 98:\n",
      "generator loss -3.010327 \n",
      "disciminator loss -1.379855\n",
      "\n",
      "\n",
      "Epoch 99:\n",
      "generator loss -2.905263 \n",
      "disciminator loss -1.370381\n",
      "\n",
      "\n",
      "Epoch 100:\n",
      "generator loss -2.886630 \n",
      "disciminator loss -1.370189\n",
      "\n",
      "\n",
      "Epoch 101:\n",
      "generator loss -2.963104 \n",
      "disciminator loss -1.366602\n",
      "\n",
      "\n",
      "Epoch 102:\n",
      "generator loss -2.879884 \n",
      "disciminator loss -1.353120\n",
      "\n",
      "\n",
      "Epoch 103:\n",
      "generator loss -2.894153 \n",
      "disciminator loss -1.349597\n",
      "\n",
      "\n",
      "Epoch 104:\n",
      "generator loss -2.819557 \n",
      "disciminator loss -1.343674\n",
      "\n",
      "\n",
      "Epoch 105:\n",
      "generator loss -2.851464 \n",
      "disciminator loss -1.339956\n",
      "\n",
      "\n",
      "Epoch 106:\n",
      "generator loss -2.861053 \n",
      "disciminator loss -1.335399\n",
      "\n",
      "\n",
      "Epoch 107:\n",
      "generator loss -2.910895 \n",
      "disciminator loss -1.334301\n",
      "\n",
      "\n",
      "Epoch 108:\n",
      "generator loss -2.939157 \n",
      "disciminator loss -1.322215\n",
      "\n",
      "\n",
      "Epoch 109:\n",
      "generator loss -2.946507 \n",
      "disciminator loss -1.321071\n",
      "\n",
      "\n",
      "Epoch 110:\n",
      "generator loss -2.896780 \n",
      "disciminator loss -1.313275\n",
      "\n",
      "\n",
      "Epoch 111:\n",
      "generator loss -3.109052 \n",
      "disciminator loss -1.312298\n",
      "\n",
      "\n",
      "Epoch 112:\n",
      "generator loss -3.104633 \n",
      "disciminator loss -1.302612\n",
      "\n",
      "\n",
      "Epoch 113:\n",
      "generator loss -2.955131 \n",
      "disciminator loss -1.302578\n",
      "\n",
      "\n",
      "Epoch 114:\n",
      "generator loss -2.983695 \n",
      "disciminator loss -1.296093\n",
      "\n",
      "\n",
      "Epoch 115:\n",
      "generator loss -3.015218 \n",
      "disciminator loss -1.290623\n",
      "\n",
      "\n",
      "Epoch 116:\n",
      "generator loss -3.078566 \n",
      "disciminator loss -1.292004\n",
      "\n",
      "\n",
      "Epoch 117:\n",
      "generator loss -3.059480 \n",
      "disciminator loss -1.285437\n",
      "\n",
      "\n",
      "Epoch 118:\n",
      "generator loss -3.121217 \n",
      "disciminator loss -1.281168\n",
      "\n",
      "\n",
      "Epoch 119:\n",
      "generator loss -3.157058 \n",
      "disciminator loss -1.276758\n",
      "\n",
      "\n",
      "Epoch 120:\n",
      "generator loss -3.130010 \n",
      "disciminator loss -1.270063\n",
      "\n",
      "\n",
      "Epoch 121:\n",
      "generator loss -3.021768 \n",
      "disciminator loss -1.269128\n",
      "\n",
      "\n",
      "Epoch 122:\n",
      "generator loss -3.126975 \n",
      "disciminator loss -1.266556\n",
      "\n",
      "\n",
      "Epoch 123:\n",
      "generator loss -3.241621 \n",
      "disciminator loss -1.260174\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124:\n",
      "generator loss -3.352531 \n",
      "disciminator loss -1.259592\n",
      "\n",
      "\n",
      "Epoch 125:\n",
      "generator loss -3.262145 \n",
      "disciminator loss -1.253771\n",
      "\n",
      "\n",
      "Epoch 126:\n",
      "generator loss -3.394707 \n",
      "disciminator loss -1.252502\n",
      "\n",
      "\n",
      "Epoch 127:\n",
      "generator loss -3.189027 \n",
      "disciminator loss -1.243811\n",
      "\n",
      "\n",
      "Epoch 128:\n",
      "generator loss -3.296335 \n",
      "disciminator loss -1.246486\n",
      "\n",
      "\n",
      "Epoch 129:\n",
      "generator loss -3.219790 \n",
      "disciminator loss -1.244708\n",
      "\n",
      "\n",
      "Epoch 130:\n",
      "generator loss -3.216094 \n",
      "disciminator loss -1.236651\n",
      "\n",
      "\n",
      "Epoch 131:\n",
      "generator loss -3.172768 \n",
      "disciminator loss -1.227468\n",
      "\n",
      "\n",
      "Epoch 132:\n",
      "generator loss -3.222558 \n",
      "disciminator loss -1.226148\n",
      "\n",
      "\n",
      "Epoch 133:\n",
      "generator loss -3.322857 \n",
      "disciminator loss -1.231474\n",
      "\n",
      "\n",
      "Epoch 134:\n",
      "generator loss -3.313283 \n",
      "disciminator loss -1.221389\n",
      "\n",
      "\n",
      "Epoch 135:\n",
      "generator loss -3.250246 \n",
      "disciminator loss -1.221906\n",
      "\n",
      "\n",
      "Epoch 136:\n",
      "generator loss -3.394973 \n",
      "disciminator loss -1.215354\n",
      "\n",
      "\n",
      "Epoch 137:\n",
      "generator loss -3.312920 \n",
      "disciminator loss -1.212307\n",
      "\n",
      "\n",
      "Epoch 138:\n",
      "generator loss -3.322819 \n",
      "disciminator loss -1.209148\n",
      "\n",
      "\n",
      "Epoch 139:\n",
      "generator loss -3.287996 \n",
      "disciminator loss -1.202682\n",
      "\n",
      "\n",
      "Epoch 140:\n",
      "generator loss -3.302481 \n",
      "disciminator loss -1.202375\n",
      "\n",
      "\n",
      "Epoch 141:\n",
      "generator loss -3.371554 \n",
      "disciminator loss -1.199424\n",
      "\n",
      "\n",
      "Epoch 142:\n",
      "generator loss -3.287888 \n",
      "disciminator loss -1.192790\n",
      "\n",
      "\n",
      "Epoch 143:\n",
      "generator loss -3.309423 \n",
      "disciminator loss -1.191541\n",
      "\n",
      "\n",
      "Epoch 144:\n",
      "generator loss -3.268513 \n",
      "disciminator loss -1.190588\n",
      "\n",
      "\n",
      "Epoch 145:\n",
      "generator loss -3.352371 \n",
      "disciminator loss -1.184855\n",
      "\n",
      "\n",
      "Epoch 146:\n",
      "generator loss -3.481310 \n",
      "disciminator loss -1.181969\n",
      "\n",
      "\n",
      "Epoch 147:\n",
      "generator loss -3.509672 \n",
      "disciminator loss -1.179936\n",
      "\n",
      "\n",
      "Epoch 148:\n",
      "generator loss -3.563550 \n",
      "disciminator loss -1.180567\n",
      "\n",
      "\n",
      "Epoch 149:\n",
      "generator loss -3.426461 \n",
      "disciminator loss -1.173514\n",
      "\n",
      "\n",
      "Epoch 150:\n",
      "generator loss -3.511668 \n",
      "disciminator loss -1.169629\n",
      "\n",
      "\n",
      "Epoch 151:\n",
      "generator loss -3.499582 \n",
      "disciminator loss -1.169484\n",
      "\n",
      "\n",
      "Epoch 152:\n",
      "generator loss -3.564084 \n",
      "disciminator loss -1.167950\n",
      "\n",
      "\n",
      "Epoch 153:\n",
      "generator loss -3.528815 \n",
      "disciminator loss -1.165250\n",
      "\n",
      "\n",
      "Epoch 154:\n",
      "generator loss -3.485389 \n",
      "disciminator loss -1.160172\n",
      "\n",
      "\n",
      "Epoch 155:\n",
      "generator loss -3.593756 \n",
      "disciminator loss -1.157490\n",
      "\n",
      "\n",
      "Epoch 156:\n",
      "generator loss -3.490446 \n",
      "disciminator loss -1.153445\n",
      "\n",
      "\n",
      "Epoch 157:\n",
      "generator loss -3.504532 \n",
      "disciminator loss -1.152564\n",
      "\n",
      "\n",
      "Epoch 158:\n",
      "generator loss -3.638387 \n",
      "disciminator loss -1.153837\n",
      "\n",
      "\n",
      "Epoch 159:\n",
      "generator loss -3.576922 \n",
      "disciminator loss -1.148695\n",
      "\n",
      "\n",
      "Epoch 160:\n",
      "generator loss -3.587233 \n",
      "disciminator loss -1.144641\n",
      "\n",
      "\n",
      "Epoch 161:\n",
      "generator loss -3.573144 \n",
      "disciminator loss -1.146441\n",
      "\n",
      "\n",
      "Epoch 162:\n",
      "generator loss -3.612501 \n",
      "disciminator loss -1.138145\n",
      "\n",
      "\n",
      "Epoch 163:\n",
      "generator loss -3.639331 \n",
      "disciminator loss -1.137084\n",
      "\n",
      "\n",
      "Epoch 164:\n",
      "generator loss -3.514874 \n",
      "disciminator loss -1.135382\n",
      "\n",
      "\n",
      "Epoch 165:\n",
      "generator loss -3.665756 \n",
      "disciminator loss -1.129081\n",
      "\n",
      "\n",
      "Epoch 166:\n",
      "generator loss -3.572193 \n",
      "disciminator loss -1.128693\n",
      "\n",
      "\n",
      "Epoch 167:\n",
      "generator loss -3.604387 \n",
      "disciminator loss -1.134761\n",
      "\n",
      "\n",
      "Epoch 168:\n",
      "generator loss -3.668995 \n",
      "disciminator loss -1.125578\n",
      "\n",
      "\n",
      "Epoch 169:\n",
      "generator loss -3.782158 \n",
      "disciminator loss -1.124327\n",
      "\n",
      "\n",
      "Epoch 170:\n",
      "generator loss -3.656495 \n",
      "disciminator loss -1.119913\n",
      "\n",
      "\n",
      "Epoch 171:\n",
      "generator loss -3.719930 \n",
      "disciminator loss -1.123292\n",
      "\n",
      "\n",
      "Epoch 172:\n",
      "generator loss -3.814161 \n",
      "disciminator loss -1.116828\n",
      "\n",
      "\n",
      "Epoch 173:\n",
      "generator loss -3.756998 \n",
      "disciminator loss -1.119830\n",
      "\n",
      "\n",
      "Epoch 174:\n",
      "generator loss -3.939992 \n",
      "disciminator loss -1.113771\n",
      "\n",
      "\n",
      "Epoch 175:\n",
      "generator loss -3.931102 \n",
      "disciminator loss -1.110304\n",
      "\n",
      "\n",
      "Epoch 176:\n",
      "generator loss -3.837193 \n",
      "disciminator loss -1.108241\n",
      "\n",
      "\n",
      "Epoch 177:\n",
      "generator loss -3.911023 \n",
      "disciminator loss -1.113015\n",
      "\n",
      "\n",
      "Epoch 178:\n",
      "generator loss -3.878169 \n",
      "disciminator loss -1.099456\n",
      "\n",
      "\n",
      "Epoch 179:\n",
      "generator loss -3.726977 \n",
      "disciminator loss -1.098870\n",
      "\n",
      "\n",
      "Epoch 180:\n",
      "generator loss -3.764987 \n",
      "disciminator loss -1.100203\n",
      "\n",
      "\n",
      "Epoch 181:\n",
      "generator loss -3.889647 \n",
      "disciminator loss -1.097741\n",
      "\n",
      "\n",
      "Epoch 182:\n",
      "generator loss -3.876553 \n",
      "disciminator loss -1.095758\n",
      "\n",
      "\n",
      "Epoch 183:\n",
      "generator loss -3.740650 \n",
      "disciminator loss -1.095188\n",
      "\n",
      "\n",
      "Epoch 184:\n",
      "generator loss -3.680857 \n",
      "disciminator loss -1.088132\n",
      "\n",
      "\n",
      "Epoch 185:\n",
      "generator loss -3.761476 \n",
      "disciminator loss -1.090850\n",
      "\n",
      "\n",
      "Epoch 186:\n",
      "generator loss -3.764879 \n",
      "disciminator loss -1.087687\n",
      "\n",
      "\n",
      "Epoch 187:\n",
      "generator loss -3.872957 \n",
      "disciminator loss -1.088772\n",
      "\n",
      "\n",
      "Epoch 188:\n",
      "generator loss -3.885418 \n",
      "disciminator loss -1.085714\n",
      "\n",
      "\n",
      "Epoch 189:\n",
      "generator loss -3.773009 \n",
      "disciminator loss -1.081863\n",
      "\n",
      "\n",
      "Epoch 190:\n",
      "generator loss -4.007922 \n",
      "disciminator loss -1.079182\n",
      "\n",
      "\n",
      "Epoch 191:\n",
      "generator loss -4.022694 \n",
      "disciminator loss -1.075753\n",
      "\n",
      "\n",
      "Epoch 192:\n",
      "generator loss -3.877654 \n",
      "disciminator loss -1.071535\n",
      "\n",
      "\n",
      "Epoch 193:\n",
      "generator loss -3.935600 \n",
      "disciminator loss -1.070848\n",
      "\n",
      "\n",
      "Epoch 194:\n",
      "generator loss -3.826646 \n",
      "disciminator loss -1.071080\n",
      "\n",
      "\n",
      "Epoch 195:\n",
      "generator loss -3.933853 \n",
      "disciminator loss -1.067290\n",
      "\n",
      "\n",
      "Epoch 196:\n",
      "generator loss -3.872446 \n",
      "disciminator loss -1.066119\n",
      "\n",
      "\n",
      "Epoch 197:\n",
      "generator loss -3.896022 \n",
      "disciminator loss -1.071521\n",
      "\n",
      "\n",
      "Epoch 198:\n",
      "generator loss -3.853186 \n",
      "disciminator loss -1.063576\n",
      "\n",
      "\n",
      "Epoch 199:\n",
      "generator loss -3.880630 \n",
      "disciminator loss -1.063817\n",
      "\n",
      "\n",
      "Epoch 200:\n",
      "generator loss -3.917822 \n",
      "disciminator loss -1.058725\n",
      "\n",
      "\n",
      "Epoch 201:\n",
      "generator loss -3.857820 \n",
      "disciminator loss -1.054988\n",
      "\n",
      "\n",
      "Epoch 202:\n",
      "generator loss -3.948845 \n",
      "disciminator loss -1.056495\n",
      "\n",
      "\n",
      "Epoch 203:\n",
      "generator loss -3.842613 \n",
      "disciminator loss -1.055510\n",
      "\n",
      "\n",
      "Epoch 204:\n",
      "generator loss -3.953132 \n",
      "disciminator loss -1.056468\n",
      "\n",
      "\n",
      "Epoch 205:\n",
      "generator loss -3.995235 \n",
      "disciminator loss -1.053673\n",
      "\n",
      "\n",
      "Epoch 206:\n",
      "generator loss -4.002630 \n",
      "disciminator loss -1.054434\n",
      "\n",
      "\n",
      "Epoch 207:\n",
      "generator loss -3.950478 \n",
      "disciminator loss -1.050957\n",
      "\n",
      "\n",
      "Epoch 208:\n",
      "generator loss -4.032005 \n",
      "disciminator loss -1.041708\n",
      "\n",
      "\n",
      "Epoch 209:\n",
      "generator loss -4.028006 \n",
      "disciminator loss -1.042640\n",
      "\n",
      "\n",
      "Epoch 210:\n",
      "generator loss -3.926636 \n",
      "disciminator loss -1.045769\n",
      "\n",
      "\n",
      "Epoch 211:\n",
      "generator loss -3.998647 \n",
      "disciminator loss -1.039938\n",
      "\n",
      "\n",
      "Epoch 212:\n",
      "generator loss -4.015824 \n",
      "disciminator loss -1.044798\n",
      "\n",
      "\n",
      "Epoch 213:\n",
      "generator loss -3.915648 \n",
      "disciminator loss -1.038063\n",
      "\n",
      "\n",
      "Epoch 214:\n",
      "generator loss -4.092398 \n",
      "disciminator loss -1.034823\n",
      "\n",
      "\n",
      "Epoch 215:\n",
      "generator loss -4.137124 \n",
      "disciminator loss -1.038548\n",
      "\n",
      "\n",
      "Epoch 216:\n",
      "generator loss -4.092580 \n",
      "disciminator loss -1.037986\n",
      "\n",
      "\n",
      "Epoch 217:\n",
      "generator loss -4.144652 \n",
      "disciminator loss -1.035800\n",
      "\n",
      "\n",
      "Epoch 218:\n",
      "generator loss -4.101909 \n",
      "disciminator loss -1.035083\n",
      "\n",
      "\n",
      "Epoch 219:\n",
      "generator loss -4.183546 \n",
      "disciminator loss -1.032178\n",
      "\n",
      "\n",
      "Epoch 220:\n",
      "generator loss -3.999119 \n",
      "disciminator loss -1.028498\n",
      "\n",
      "\n",
      "Epoch 221:\n",
      "generator loss -4.038817 \n",
      "disciminator loss -1.027216\n",
      "\n",
      "\n",
      "Epoch 222:\n",
      "generator loss -4.000748 \n",
      "disciminator loss -1.025094\n",
      "\n",
      "\n",
      "Epoch 223:\n",
      "generator loss -4.226721 \n",
      "disciminator loss -1.027376\n",
      "\n",
      "\n",
      "Epoch 224:\n",
      "generator loss -3.989601 \n",
      "disciminator loss -1.021395\n",
      "\n",
      "\n",
      "Epoch 225:\n",
      "generator loss -4.132558 \n",
      "disciminator loss -1.025815\n",
      "\n",
      "\n",
      "Epoch 226:\n",
      "generator loss -3.951572 \n",
      "disciminator loss -1.019710\n",
      "\n",
      "\n",
      "Epoch 227:\n",
      "generator loss -3.845548 \n",
      "disciminator loss -1.020136\n",
      "\n",
      "\n",
      "Epoch 228:\n",
      "generator loss -3.912078 \n",
      "disciminator loss -1.017446\n",
      "\n",
      "\n",
      "Epoch 229:\n",
      "generator loss -3.862364 \n",
      "disciminator loss -1.016788\n",
      "\n",
      "\n",
      "Epoch 230:\n",
      "generator loss -3.876571 \n",
      "disciminator loss -1.016090\n",
      "\n",
      "\n",
      "Epoch 231:\n",
      "generator loss -3.971419 \n",
      "disciminator loss -1.013409\n",
      "\n",
      "\n",
      "Epoch 232:\n",
      "generator loss -4.025615 \n",
      "disciminator loss -1.010615\n",
      "\n",
      "\n",
      "Epoch 233:\n",
      "generator loss -3.974201 \n",
      "disciminator loss -1.012656\n",
      "\n",
      "\n",
      "Epoch 234:\n",
      "generator loss -4.061087 \n",
      "disciminator loss -1.011011\n",
      "\n",
      "\n",
      "Epoch 235:\n",
      "generator loss -3.941887 \n",
      "disciminator loss -1.007779\n",
      "\n",
      "\n",
      "Epoch 236:\n",
      "generator loss -4.000096 \n",
      "disciminator loss -1.012417\n",
      "\n",
      "\n",
      "Epoch 237:\n",
      "generator loss -4.053936 \n",
      "disciminator loss -1.004429\n",
      "\n",
      "\n",
      "Epoch 238:\n",
      "generator loss -3.901669 \n",
      "disciminator loss -1.005919\n",
      "\n",
      "\n",
      "Epoch 239:\n",
      "generator loss -4.018702 \n",
      "disciminator loss -1.005318\n",
      "\n",
      "\n",
      "Epoch 240:\n",
      "generator loss -4.119706 \n",
      "disciminator loss -1.003618\n",
      "\n",
      "\n",
      "Epoch 241:\n",
      "generator loss -4.078007 \n",
      "disciminator loss -1.005640\n",
      "\n",
      "\n",
      "Epoch 242:\n",
      "generator loss -4.013031 \n",
      "disciminator loss -1.000013\n",
      "\n",
      "\n",
      "Epoch 243:\n",
      "generator loss -4.207562 \n",
      "disciminator loss -1.002989\n",
      "\n",
      "\n",
      "Epoch 244:\n",
      "generator loss -4.096976 \n",
      "disciminator loss -0.996831\n",
      "\n",
      "\n",
      "Epoch 245:\n",
      "generator loss -4.186349 \n",
      "disciminator loss -0.998332\n",
      "\n",
      "\n",
      "Epoch 246:\n",
      "generator loss -4.085977 \n",
      "disciminator loss -0.995722\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247:\n",
      "generator loss -3.994723 \n",
      "disciminator loss -0.998052\n",
      "\n",
      "\n",
      "Epoch 248:\n",
      "generator loss -4.087296 \n",
      "disciminator loss -0.993855\n",
      "\n",
      "\n",
      "Epoch 249:\n",
      "generator loss -4.107032 \n",
      "disciminator loss -0.996128\n",
      "\n",
      "\n",
      "Epoch 250:\n",
      "generator loss -4.036721 \n",
      "disciminator loss -0.991572\n",
      "\n",
      "\n",
      "Epoch 251:\n",
      "generator loss -4.112561 \n",
      "disciminator loss -0.991716\n",
      "\n",
      "\n",
      "Epoch 252:\n",
      "generator loss -4.006261 \n",
      "disciminator loss -0.989514\n",
      "\n",
      "\n",
      "Epoch 253:\n",
      "generator loss -3.926241 \n",
      "disciminator loss -0.987627\n",
      "\n",
      "\n",
      "Epoch 254:\n",
      "generator loss -4.093718 \n",
      "disciminator loss -0.987920\n",
      "\n",
      "\n",
      "Epoch 255:\n",
      "generator loss -3.953583 \n",
      "disciminator loss -0.986394\n",
      "\n",
      "\n",
      "Epoch 256:\n",
      "generator loss -4.215789 \n",
      "disciminator loss -0.985449\n",
      "\n",
      "\n",
      "Epoch 257:\n",
      "generator loss -4.210157 \n",
      "disciminator loss -0.986635\n",
      "\n",
      "\n",
      "Epoch 258:\n",
      "generator loss -4.019529 \n",
      "disciminator loss -0.981370\n",
      "\n",
      "\n",
      "Epoch 259:\n",
      "generator loss -4.072939 \n",
      "disciminator loss -0.985502\n",
      "\n",
      "\n",
      "Epoch 260:\n",
      "generator loss -4.107701 \n",
      "disciminator loss -0.984343\n",
      "\n",
      "\n",
      "Epoch 261:\n",
      "generator loss -4.118053 \n",
      "disciminator loss -0.980145\n",
      "\n",
      "\n",
      "Epoch 262:\n",
      "generator loss -4.065209 \n",
      "disciminator loss -0.977845\n",
      "\n",
      "\n",
      "Epoch 263:\n",
      "generator loss -3.975117 \n",
      "disciminator loss -0.980709\n",
      "\n",
      "\n",
      "Epoch 264:\n",
      "generator loss -4.100341 \n",
      "disciminator loss -0.979558\n",
      "\n",
      "\n",
      "Epoch 265:\n",
      "generator loss -4.088387 \n",
      "disciminator loss -0.974405\n",
      "\n",
      "\n",
      "Epoch 266:\n",
      "generator loss -4.108388 \n",
      "disciminator loss -0.974650\n",
      "\n",
      "\n",
      "Epoch 267:\n",
      "generator loss -4.056479 \n",
      "disciminator loss -0.977338\n",
      "\n",
      "\n",
      "Epoch 268:\n",
      "generator loss -3.969727 \n",
      "disciminator loss -0.977646\n",
      "\n",
      "\n",
      "Epoch 269:\n",
      "generator loss -4.050095 \n",
      "disciminator loss -0.973887\n",
      "\n",
      "\n",
      "Epoch 270:\n",
      "generator loss -4.191000 \n",
      "disciminator loss -0.972740\n",
      "\n",
      "\n",
      "Epoch 271:\n",
      "generator loss -4.038397 \n",
      "disciminator loss -0.975496\n",
      "\n",
      "\n",
      "Epoch 272:\n",
      "generator loss -4.002071 \n",
      "disciminator loss -0.974650\n",
      "\n",
      "\n",
      "Epoch 273:\n",
      "generator loss -4.188068 \n",
      "disciminator loss -0.970966\n",
      "\n",
      "\n",
      "Epoch 274:\n",
      "generator loss -4.066724 \n",
      "disciminator loss -0.971741\n",
      "\n",
      "\n",
      "Epoch 275:\n",
      "generator loss -4.065892 \n",
      "disciminator loss -0.969780\n",
      "\n",
      "\n",
      "Epoch 276:\n",
      "generator loss -4.170890 \n",
      "disciminator loss -0.970384\n",
      "\n",
      "\n",
      "Epoch 277:\n",
      "generator loss -4.125706 \n",
      "disciminator loss -0.970045\n",
      "\n",
      "\n",
      "Epoch 278:\n",
      "generator loss -4.191984 \n",
      "disciminator loss -0.964550\n",
      "\n",
      "\n",
      "Epoch 279:\n",
      "generator loss -4.216460 \n",
      "disciminator loss -0.970073\n",
      "\n",
      "\n",
      "Epoch 280:\n",
      "generator loss -4.068547 \n",
      "disciminator loss -0.964985\n",
      "\n",
      "\n",
      "Epoch 281:\n",
      "generator loss -4.232298 \n",
      "disciminator loss -0.967712\n",
      "\n",
      "\n",
      "Epoch 282:\n",
      "generator loss -4.133527 \n",
      "disciminator loss -0.969867\n",
      "\n",
      "\n",
      "Epoch 283:\n",
      "generator loss -4.166615 \n",
      "disciminator loss -0.965104\n",
      "\n",
      "\n",
      "Epoch 284:\n",
      "generator loss -4.120219 \n",
      "disciminator loss -0.966339\n",
      "\n",
      "\n",
      "Epoch 285:\n",
      "generator loss -4.061858 \n",
      "disciminator loss -0.964983\n",
      "\n",
      "\n",
      "Epoch 286:\n",
      "generator loss -4.186681 \n",
      "disciminator loss -0.960364\n",
      "\n",
      "\n",
      "Epoch 287:\n",
      "generator loss -4.243356 \n",
      "disciminator loss -0.959922\n",
      "\n",
      "\n",
      "Epoch 288:\n",
      "generator loss -4.072578 \n",
      "disciminator loss -0.957956\n",
      "\n",
      "\n",
      "Epoch 289:\n",
      "generator loss -4.138339 \n",
      "disciminator loss -0.957484\n",
      "\n",
      "\n",
      "Epoch 290:\n",
      "generator loss -4.090870 \n",
      "disciminator loss -0.963519\n",
      "\n",
      "\n",
      "Epoch 291:\n",
      "generator loss -4.097471 \n",
      "disciminator loss -0.960597\n",
      "\n",
      "\n",
      "Epoch 292:\n",
      "generator loss -4.042541 \n",
      "disciminator loss -0.960646\n",
      "\n",
      "\n",
      "Epoch 293:\n",
      "generator loss -4.121228 \n",
      "disciminator loss -0.956759\n",
      "\n",
      "\n",
      "Epoch 294:\n",
      "generator loss -4.049482 \n",
      "disciminator loss -0.959822\n",
      "\n",
      "\n",
      "Epoch 295:\n",
      "generator loss -4.131887 \n",
      "disciminator loss -0.955497\n",
      "\n",
      "\n",
      "Epoch 296:\n",
      "generator loss -4.146807 \n",
      "disciminator loss -0.955582\n",
      "\n",
      "\n",
      "Epoch 297:\n",
      "generator loss -4.116514 \n",
      "disciminator loss -0.957134\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wlg = [None] * W_EPOCH #record loss of g for each epoch\n",
    "wld = [None] * W_EPOCH #record loss of d for each epoch\n",
    "wsp = [None] * W_EPOCH #record sample images for each epoch\n",
    "\n",
    "rsTrain = float(BATCH_SIZE) / float(len(file_list))\n",
    "ctr = 0\n",
    "for ep in range(W_EPOCH):\n",
    "    lgt = 0.0\n",
    "    ldt = 0.0\n",
    "    for c1 in dataset:\n",
    "        if(c1.shape[0] == BATCH_SIZE):\n",
    "            lg, ld = WTrain[ctr](c1)\n",
    "            ctr += 1\n",
    "            lgt += lg.numpy()\n",
    "            ldt += ld.numpy()\n",
    "            if ctr == WCritic : ctr = 0\n",
    "    wlg[ep] = lgt * rsTrain\n",
    "    wld[ep] = ldt * rsTrain\n",
    "    print(\"Epoch %d:\" % ep)\n",
    "    print(\"generator loss %3f \" % wlg[ep])\n",
    "    print(\"disciminator loss %3f\" % wld[ep])\n",
    "    print(\"\\n\")\n",
    "    out = WG(s, training = False)\n",
    "    img = utPuzzle(\n",
    "        (out * 255.0).numpy().astype(np.uint8),\n",
    "        SAMPLE_COL,\n",
    "        SAMPLE_ROW,\n",
    "        \"imgs/w_%04d.png\" % ep\n",
    "    )\n",
    "    wsp[ep] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utMakeGif(np.array(wsp), \"imgs/wgan.gif\", duration = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(W_EPOCH), wld, color = \"blue\", label = \"Discriminator Loss\")\n",
    "plt.plot(range(W_EPOCH), wlg, color = \"red\", label = \"Generator Loss\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"WGAN Training Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
